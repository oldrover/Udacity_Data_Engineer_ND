{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, dayofweek, date_format, monotonically_increasing_id\n",
    "from pyspark.sql.types import TimestampType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Read config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "SONG_DATA_SET=config['TEST']['SONG_DATA_SET']\n",
    "LOG_DATA_SET=config['TEST']['LOG_DATA_SET']\n",
    "OUTPUT_DATA=config['TEST']['OUTPUT_DATA']\n",
    "\n",
    "print(SONG_DATA_SET)\n",
    "print(LOG_DATA_SET)\n",
    "print(OUTPUT_DATA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Read song data and write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_songdata = spark.read.json(SONG_DATA_SET)\n",
    "df_songdata.createOrReplaceTempView(\"df_songs_table\")\n",
    "\n",
    "#df_songdata.printSchema()\n",
    "#df_songdata.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songs_table = df_songdata.select(\"song_id\",\"title\",\"artist_id\",\"year\",\"duration\").drop_duplicates()\n",
    "\n",
    "#songs_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songs_table.write.parquet(OUTPUT_DATA + \"songs/\", mode=\"overwrite\", partitionBy=[\"year\",\"artist_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create artists table and write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "artists_table = df_songdata.select(\"artist_id\",\"artist_name\",\"artist_location\",\"artist_latitude\",\"artist_longitude\").drop_duplicates()\n",
    "\n",
    "#artists_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "artists_table.write.parquet(OUTPUT_DATA + \"artists/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Read log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_logdata = spark.read.json(LOG_DATA_SET)\n",
    "\n",
    "#df_logdata.printSchema()\n",
    "#df_logdata.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_logdata = df_logdata.filter(df_logdata.page == \"NextSong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create Users table and write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "users_table = df_logdata.select(\"userId\",\"firstName\",\"lastName\",\"gender\",\"level\").drop_duplicates()\n",
    "\n",
    "#users_table.printSchema()\n",
    "#users_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "users_table.write.parquet(OUTPUT_DATA + \"users/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Add timestamp column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "get_timestamp = udf(lambda ts : datetime.utcfromtimestamp(int(ts)/1000), TimestampType())   \n",
    "df = df_logdata.withColumn(\"timestamp\", get_timestamp(col(\"ts\")))\n",
    "\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Add datetime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "get_datetime = udf(lambda ts: to_date(ts), TimestampType())\n",
    "df = df.withColumn(\"start_time\", get_timestamp(col(\"ts\")))\n",
    "\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"hour\",hour(\"start_time\"))\\\n",
    "        .withColumn(\"day\",dayofmonth(\"start_time\"))\\\n",
    "        .withColumn(\"week\",weekofyear(\"start_time\"))\\\n",
    "        .withColumn(\"month\",month(\"start_time\"))\\\n",
    "        .withColumn(\"year\",year(\"start_time\"))\\\n",
    "        .withColumn(\"weekday\",dayofweek(\"start_time\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create time table and write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_table = df.select(\"start_time\", \"hour\", \"day\", \"week\", \"month\", \"year\", \"weekday\").distinct()\n",
    "\n",
    "#time_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_table.write.parquet(OUTPUT_DATA + \"time_table/\", mode=\"overwrite\", partitionBy=[\"year\",\"month\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create songplays table and write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_df = spark.sql(\"SELECT DISTINCT song_id, artist_id, artist_name FROM df_songs_table\")\n",
    "\n",
    "#song_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplays_table = df.join(song_df, song_df.artist_name == df.artist, \"inner\")\\\n",
    "        .distinct()\\\n",
    "        .select(\"start_time\", \"userId\", \"level\", \"sessionId\", \"location\", \"userAgent\",\"song_id\",\"artist_id\", \"month\", \"year\")\\\n",
    "        .withColumn(\"songplay_id\", monotonically_increasing_id())\n",
    "\n",
    "#songplays_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplays_table.write.parquet(OUTPUT_DATA + \"songplays_table/\", mode=\"overwrite\", partitionBy=[\"year\", \"month\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Read songplays table without partitioning columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "read_df = spark.read.parquet(OUTPUT_DATA + \"songplays_table/*/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "read_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "read_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
